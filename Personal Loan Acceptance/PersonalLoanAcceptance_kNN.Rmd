```{r}
#Consider the following customer: Age=40, Experience=10, Income=84, Family=2, CCAvg=2, Education2=1, Education3=0, Mortgage=O, Securities Account=O, CD Account=O, Online=1 and Credit.card = 1. Perform a k-NN classification with all predictors except ID and ZIP code using k = 1. Remember to transform categorical predictors with more than two categories into dummy variables first. Specify the success class as 1 (loan acceptance), and use the default cutoff value of 0.5. How would this customer be classified?

library(readxl)
library(caret)
library(lattice)
library(class)
UB<-read_excel("UniversalBank.xlsx",sheet="Data")
#According to the question, we need to add two columns education2 and education 3 also we need to make the education2=1, education3=0 
UB$Education1<-- (ifelse(UB[,8]==1,1,0))
UB$Education2 <- (ifelse(UB[,8]==2,1,0))
UB$Education3 <- (ifelse(UB[,8]==3,1,0))
#According to the requirment the first step is to partition the data into trainning data and Validation data 
trainingUB<- sample(row.names(UB), 0.6*dim(UB)[1])
ValidationUB<- setdiff(row.names(UB), trainingUB)
training.df <- UB[trainingUB,]
validation.df <- UB[ValidationUB,]

#View(UB)
#we also need to remove the columns we do not need 

training.new.df <- training.df[,-which(names(training.df) %in% c("ID","ZIP Code","Education"))]
validation.new.df <- validation.df[,-which(names(validation.df) %in% c("ID","ZIP Code","Education"))]

TS.df <- data.frame('Age'=40, 'Experience'=10, 'Income'=84, 'Family'=2, 
                           'CCAvg'=2,'Mortgage'=0, 'Education1'=0, 'Education2'=1, 
                           'Education3'=0,'Securities.Account'=0, 'CD.Account'=0,
                           'Online'=1,'CreditCard'= 1)

TS.norm <- TS.df
norm <- preProcess(training.new.df[,c(-7:-13)],method = c("center", "scale"))
TS.norm[,c(1:6)] <- predict(norm, TS.norm[,c(1:6)])
TS.norm
UB$`Personal Loan`<-factor(UB$`Personal Loan`)
Knn.test<-knn(train=training.new.df[,-7],test=TS.norm,cl=training.new.df$`Personal Loan`,k=1)
Knn.test

```
So, for the given data, we have a 0.

#Finding a value for k that balances between overfitting and ignoring the predictor information
```{r}
library(caret)
library(class)
library(lattice)
library(readxl)

UB<-read_excel("UniversalBank.xlsx",sheet="Data")
trainingUB<- sample(row.names(UB), 0.6*dim(UB)[1])
ValidationUB<- setdiff(row.names(UB), trainingUB)
training.df <- UB[trainingUB,]
validation.df <- UB[ValidationUB,]
training.new.df <- training.df[,-which(names(training.df) %in% c("ID","ZIP Code","Education"))]
validation.new.df <- validation.df[,-which(names(validation.df) %in% c("ID","ZIP Code","Education"))]
set.seed(5)
accuracy.a <- data.frame(k=seq(1,10,1),accuracy=rep(0,10))
for(i in c(1:10)){
   knn.pred<-knn(training.new.df[,-7],validation.new.df[,-7],
                 cl=training.new.df$`Personal Loan`,k = i)
                 accuracy.a[i,2]<-confusionMatrix(knn.pred,factor(validation.new.df$`Personal Loan`))$overall[1]
}

accuracy.a
plot(accuracy.a, type = "b")


 
``` 
Classification matrix for the validation data that results from using the best k.

k = 3
```{r}
pred_knn<-knn(training.new.df[,-7],validation.new.df[,-7],
                 cl=training.new.df$`Personal Loan`,k = 3)
pred_knn
confusionMatrix(pred_knn,factor(validation.new.df$`Personal Loan`))
```
Consider the following customer: Age=40, Experience=10, Income=84, Family=2, CCAvg=2, Education 1=0, Education 2=1, Education 3=0, Mortgage=0, Securities Account=0, CD Account=0, Online=1 and Credit Card=1. Classify the customer using the best k.

k = 3
```{r}
knn_test<-knn(train=training.new.df[,-7],test = validation.new.df[,-7],cl=training.new.df$`Personal Loan`, k=3)
knn_test
```
# Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. Compare the classification matrix of the test set with that of the training and validation sets. Comment on the differences and their reason
#  Subset the data into traing,validation and test sets
```{r}
index_train <- sample(row.names(UB), 0.5*dim(UB)[1])
index_valid<- sample(row.names(UB), 0.3*dim(UB)[1])
index_test<- sample(row.names(UB), 0.2*dim(UB)[1])
df_train <- UB[index_train,]
df_valid<- UB[index_valid,]
df_test<- UB[index_test,]
```
# Remove variables 
```{r}
train_e<-df_train[,-which(names(df_train) %in% c("ID","ZIP Code","Education"))]
valid_e<-df_valid[,-which(names(df_valid) %in% c("ID","ZIP Code","Education"))]
test_e<-df_test[,-which(names(df_test) %in% c("ID","ZIP Code","Education"))]
```
# data normalization
```{r}
train_E<-train_e
valid_E<-valid_e
test_E<-test_e
train_E[1:6]<-as.data.frame(lapply(train_e[1:6],normalize))
valid_E[1:6]<-as.data.frame(lapply(valid_e[1:6],normalize))
test_E[1:6] <- as.data.frame(lapply(test_e[1:6],normalize))
```
# knn model
```{r}
knn_train <- knn(train_E[,-7],train_E[,-7],cl=train_E$`Personal Loan`,k = 3) 
knn_valid <- knn(train_E[,-7],valid_E[,-7],cl=train_E$`Personal Loan`,k = 3) 
knn_test <- knn(train_E[,-7],test_E[,-7],cl=train_E$`Personal Loan`,k = 3) 
head(knn_train)
head(knn_valid)
head(knn_test)

acc_e <- data.frame(subset=c('Train','Valid','Test'),accuracy=rep(0,3))
acc_e[1,2] <- confusionMatrix(knn_train,factor(train_E$`Personal Loan`))$overall[1]
acc_e[2,2] <- confusionMatrix(knn_valid,factor(train_E$`Personal Loan`))$overall[1]
acc_e[3,2] <- confusionMatrix(knn_test,factor(train_E$`Personal Loan`))$overall[1]
acc_e
```

When k=3, accuracy is equal to 0.903 




```{r}
#Problem 3

library(readxl)
Acc <- read_excel("Accidents.xlsx", sheet = "Data")

Acc$Injury <- as.factor(ifelse(Acc$MAX_SEV_IR < 1, "NO", "YES"))
Acc1 <- Acc[,-24]

index <- sample(1:nrow(Acc), size=0.5*nrow(Acc))
test <- Acc[index,]
train<- Acc[-index,]

library(e1071)
levels(train$Injury)
model1 <- naiveBayes(Injury~., data = train)
class(model1) 
p <- predict(model1,test)
table(p)

library(e1071)
Acc2 <-Acc1[1:12,c("Injury", "WEATHER_R", "TRAF_CON_R")]
Acc2
table(Acc2$WEATHER_R, Acc2$TRAF_CON_R, Acc2$Injury)

Acc2$Injury <- ifelse(Acc2$Injury == "YES", 1, 0)
Acc2$Injury <- as.factor(Acc2$Injury)

model2 <- naiveBayes(Injury ~ WEATHER_R + TRAF_CON_R, data = Acc2)
class(model2)

model3 <- predict(model2, Acc2)
table(model3)

Acc1$Injury<-ifelse(Acc1$Injury == "YES", 1, 0)
Acc1$Injury<-as.factor(Acc1$Injury)
train1<-Acc1[1:25310, ]
valid1<-Acc1[25311:42183, ]

model4 <- naiveBayes(Injury ~ HOUR_I_R + ALCHL_I + WKDY_I_R + LGTCON_I_R + MANCOL_I_R + SPD_LIM + VEH_INVL, data=train1)
model4

p2 <- predict(model4, valid1)
table(p2, valid1$Injury)
```
A) From p2, we can see the probability of Yes is 50.87, and that of No is 49.12.

B)

```{r}
Ac.df <- read_excel("Accidents.xlsx", sheet = "Data")
Ac.df$INJURY <- ifelse(Ac.df$MAX_SEV_IR>0, "Yes", "No")
Ac.df
x.df <- Ac.df[1:12,c("INJURY","WEATHER_R","TRAF_CON_R")]
x.df
```




i)Pivot Table

```{r}
knitr::include_graphics('PT.PNG')
```
ii) Exact Bayes calculation for INJURY=Yes

```{r}
#P(Injury=yes|WEATHER_R = 1, TRAF_CON_R =0):
n1 <- 2/3 * 3/12
d1 <- 3/12
prob1 <- n1/d1

#P(Injury=yes|WEATHER_R = 1, TRAF_CON_R =1):
n2 <- 0 * 3/12
d2 <- 1/12
prob2 <- n2/d2

#P(Injury=yes| WEATHER_R = 1, TRAF_CON_R =2):
n3 <- 0 * 3/12
d3 <- 1/12
prob3 <- n3/d3

#P(Injury=yes| WEATHER_R = 2, TRAF_CON_R =0):
n4 <- 1/3 * 3/12
d4 <- 6/12
prob4 <- n4/d4

#P(Injury=yes| WEATHER_R = 2, TRAF_CON_R =1):
n5 <- 0 * 3/12
d5 <- 1/12
prob5 <- n5/d5

#P(Injury=yes| WEATHER_R = 2, TRAF_CON_R =2):
n6 <- 0 * 3/12
d6 <- 0
prob6 <- n6/d6

x<-c(1,2,3,4,5,6)
y<-c(prob1,prob2,prob3,prob4,prob5,prob6)
prob.df<-data.frame(x,y)
names(prob.df)<-c('#','Probability')
prob.df
```


(iii)Classification of accidents with cutoff=0.5:
```{r}
acc1.prob<-x.df
head(acc1.prob)

prob.injury <- c(0.667, 0.167, 0, 0, 0.667, 0.167, 0.167, 0.667, 0.167, 0.167, 0.167, 0)
#acc1.prob['PROB_INJURY'] <- NA
acc1.prob$PROB_INJURY<-prob.injury


acc1.prob$PREDICT_PROB<-ifelse(acc1.prob$PROB_INJURY>0.5,"Yes","No")
acc1.prob
```
P(Injury = YES | WEATHER_R = 1, TRAF_CON_R = 0) = Yes
P(Injury = YES | WEATHER_R = 1, TRAF_CON_R = 1) = No
P(Injury = YES | WEATHER_R = 1, TRAF_CON_R = 2) = No
P(Injury = YES | WEATHER_R = 2, TRAF_CON_R = 0) = No
P(Injury = YES | WEATHER_R = 2, TRAF_CON_R = 1) = No
P(Injury = YES | WEATHER_R = 2, TRAF_CON_R = 2) = No
P(Injury = NO | WEATHER_R = 1, TRAF_CON_R = 0) = Yes
P(Injury = NO | WEATHER_R = 1, TRAF_CON_R = 1) = No
P(Injury = NO | WEATHER_R = 1, TRAF_CON_R = 2) = No
P(Injury = NO | WEATHER_R = 2, TRAF_CON_R = 0) = Yes
P(Injury = NO | WEATHER_R = 2, TRAF_CON_R = 1) = No
P(Injury = NO | WEATHER_R = 2, TRAF_CON_R = 2) = No

iv)Naive Bayes:

```{r}
prob <- 2/3 * 0/3 * 3/12
prob
```
p(Injury=Yes|TRAF_CON_R=1, WEATHER_R=1)=0

v)
```{r}
library(e1071)
library(klaR)
library(caret)
n1<-naiveBayes(INJURY ~ ., data = x.df)
predict(n1, newdata = x.df,type = "raw")

library(caret)
o1=x.df[,-3]
o2=x.df$INJURY
cla1 <- train(o1,o2,'nb', trControl = trainControl(method = 'cv',number=10))
cla1
pred<-predict(cla1$finalModel,o1)
pred
table(pred$class,o2)

```
C)

From the following model, HOUR_I_R, ALCOHOL_I, WKDY_I_R, LGTCON_I_R,
MAN_COL_I, SPD_LIM can be included when weather conditions and location is not known.

Overall Error = (911+7275)/(7054+7275+911+1633)*100=48.51

There is no improvement in the model.

We get a probability of zero for the given conditional probability as Naive Bayes takes the probability of the category of predictor as zero.

